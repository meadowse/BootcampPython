# DAY 06 – Проект-скрапинг
## Как структурировать код
Сегодня вы создадите небольшой "реальный" проект, который будет собирать данные с веб-страниц. Для этого нужно описать несколько классов, которые будут описывать логику каждого этапа работы с данными, а также описать их взаимодействие. Каждый логический модуль принято писать в своем файле-модуле.

### Преамбула

Грамотная структура организации кода в проекте - это огромная часть работы разработчика. Важный принцип построения архитектуры проекта - это модульность. Части проекта, которые отвечают за разный функционал, не должны сильно зависеть друг от друга, чтобы изменения одного модуля минимально касались остальных. Для достижения этого принципа принято разделять логику модуля на "внутреннюю" и "внешнюю", т.е. API - протокол взаимодействия с модулем. Внешняя логика должна оставаться постоянной, в то время как внутреннюю можно менять хоть каждый день, в зависимости от требований бизнеса. Сегодня вы напишете проект - веб-скрапер - модуль, который умеет добывать и структурировать данные с веб-страниц.

### Цели

Наша цель - научить структурировать код в питоновских проектах, а также узнать, как устроена веб-страница.

### Задание

Задача скрапера - добыть данные с веб-страниц. Такой подход нужен только тогда, когда у поставщика данных нет какого-то установленного протокола скачивания данных или доступа к хранилищу данных (или всё это стоит денег, которые не вписываются в наш бюджет =) ). Логику выполнения этой задачи можно разделить на три модуля:

1. Модуль, который скачивает код веб-страницы на компьютер, чтобы обрабатывать данные с нее локально;
2. Модуль, который найдет и структурирует данные в исходном коде веб-страницы;
3. Модуль, который обрабатывает данные в соответствии с бизнес-логикой системы.

Также нам нужна точка входа в наше приложение - то место, где будет собран весь основной функционал, который мы хотим сделать доступным пользователем нашего модуля.

Наш проект небольшой - поэтому нам понадобится всего 4 файла для описания вышеперечисленных модулей. Ваша задача - написать эти 4 файла с кодом, где, соответственно, будет описана логика каждого подмодуля:

1. `download.py` - модуль, ответственный за скачивание веб-страницы. Содержит описание класса Downloader (см. блокнот с заданием).
2. `parse.py` - модуль, в котором описана логика выделения данных из исходного кода веб-страницы. Содержит описание класса Parser (см. блокнот с заданием). Рекомендуется для парсинга веб-страницы использовать библиотеку BeautifulSoup4.
3. `data.py` - модуль, в котором описана любая логика обработки полученных и структурированных файлов (на ваш выбор).
4. `__init__.py` - файл, в котором будет описана функция process, которая будет принимать на вход адрес страницы, которую мы хотим заскрапить, опциональным аргументом путь к файлу, который будет хранить копию страницы на локальном компьютере, и еще одним опциональным аргументом - путь к файлу, куда мы сохраним результаты парсинга страницы. Возвращать эта функция должна результат исполнения какой-то части логики, описанной в data.py.

### Сдача работы и проверка

Вам нужно загрузить в GIT в папку `src` папку с файлами `download.py`, `parse.py`, `data.py`, `__init__.py`, в которых будет описан код скрапера.
